import streamlit as st
import json
import pandas as pd
import numpy as np
from datetime import datetime
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—ã –ø—Ä–æ–µ–∫—Ç–∞
from universal_language_controller import (
    UniversalLanguageController,
    FieldValue,
    FieldType,
    RequestType,
    RealityType
)
from integrated_universal_system import IntegratedUniversalSystem
from vectorizer import get_vector

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Infera AGI System",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
if 'controller' not in st.session_state:
    st.session_state.controller = UniversalLanguageController()
if 'integrated_system' not in st.session_state:
    st.session_state.integrated_system = IntegratedUniversalSystem()
if 'processing_history' not in st.session_state:
    st.session_state.processing_history = []

def main():
    st.title("üß† Infera AGI System")
    st.markdown("**–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏**")
    
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    with st.sidebar:
        st.header("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã")
        
        total_objects = len(st.session_state.controller.objects)
        objects_by_type = {
            "–û–±—ä–µ–∫—Ç—ã": len(st.session_state.controller.type_index.get(RequestType.OBJECT, [])),
            "–î–µ–π—Å—Ç–≤–∏—è": len(st.session_state.controller.type_index.get(RequestType.ACTION, [])),
            "–í–æ–ø—Ä–æ—Å—ã": len(st.session_state.controller.type_index.get(RequestType.QUESTION, []))
        }
        
        st.metric("–í—Å–µ–≥–æ –æ–±—ä–µ–∫—Ç–æ–≤", total_objects)
        
        # –î–∏–∞–≥—Ä–∞–º–º–∞ –ø–æ —Ç–∏–ø–∞–º
        if total_objects > 0:
            fig_pie = px.pie(
                values=list(objects_by_type.values()),
                names=list(objects_by_type.keys()),
                title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º"
            )
            st.plotly_chart(fig_pie, use_container_width=True)
        
        # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏
        if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —Å–∏—Å—Ç–µ–º—É"):
            st.session_state.controller = UniversalLanguageController()
            st.session_state.integrated_system = IntegratedUniversalSystem()
            st.session_state.processing_history = []
            st.success("–°–∏—Å—Ç–µ–º–∞ –æ—á–∏—â–µ–Ω–∞!")
            st.rerun()
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üí¨ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞", 
        "üîç –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–∫—Ç–æ–≤", 
        "‚ö° –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞",
        "üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è",
        "üß† –û–±—É—á–µ–Ω–∏–µ"
    ])
    
    with tab1:
        process_text_tab()
    
    with tab2:
        analyze_objects_tab()
    
    with tab3:
        integrated_system_tab()
    
    with tab4:
        visualization_tab()
    
    with tab5:
        training_tab()

def process_text_tab():
    st.header("üí¨ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞")
    
    # –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    examples = [
        "–Ø —Ö–æ—á—É —Å—Ç–∞—Ç—å –º–∏–ª–ª–∏–æ–Ω–µ—Ä–æ–º",
        "–ö–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å?",
        "–£ –º–µ–Ω—è –µ—Å—Ç—å —Å—Ç–∞—Ä—Ç–∞–ø –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ò–ò",
        "–î–æ—Ö–æ–¥ –±–∏–∑–Ω–µ—Å–∞ 100000 —Ä—É–±–ª–µ–π –≤ –º–µ—Å—è—Ü",
        "–°–∫–æ–ª—å–∫–æ —ç—Ç–æ –±—É–¥–µ—Ç —Å—Ç–æ–∏—Ç—å?",
        "–í –∫–æ–º–Ω–∞—Ç–µ –µ—Å—Ç—å –¥–≤–∞ —Å—Ç—É–ª–∞",
        "–°–æ–ª–Ω—Ü–µ —Å–≤–µ—Ç–∏—Ç —è—Ä–∫–æ"
    ]
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        user_input = st.text_area(
            "–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:",
            height=100,
            placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –Ø —Ö–æ—á—É –∫—É–ø–∏—Ç—å –º–∞—à–∏–Ω—É –∑–∞ 2 –º–∏–ª–ª–∏–æ–Ω–∞ —Ä—É–±–ª–µ–π"
        )
    
    with col2:
        st.write("**–ü—Ä–∏–º–µ—Ä—ã:**")
        for i, example in enumerate(examples):
            if st.button(f"üìù {example[:20]}...", key=f"example_{i}"):
                st.session_state.example_text = example
                st.rerun()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω
    if 'example_text' in st.session_state:
        user_input = st.session_state.example_text
        del st.session_state.example_text
    
    col1, col2, col3 = st.columns([1, 1, 1])
    
    with col1:
        if st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å", type="primary"):
            if user_input.strip():
                process_user_input(user_input)
    
    with col2:
        if st.button("üîó –°–æ–∑–¥–∞—Ç—å —Å–≤—è–∑–∏"):
            create_temporal_chains()
    
    with col3:
        if st.button("‚ùì –†–∞–∑—Ä–µ—à–∏—Ç—å –≤–æ–ø—Ä–æ—Å—ã"):
            resolve_questions()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if st.session_state.processing_history:
        st.subheader("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        latest_result = st.session_state.processing_history[-1]
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        st.info(f"**–í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç:** {latest_result['input']}")
        
        # –ê–Ω–∞–ª–∏–∑
        if latest_result['analysis']:
            st.write("**üîç –ê–Ω–∞–ª–∏–∑:**")
            analysis_df = pd.DataFrame(latest_result['analysis'])
            st.dataframe(analysis_df, use_container_width=True)
        
        # –°–æ–∑–¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
        if latest_result['created_objects']:
            st.write("**üéØ –°–æ–∑–¥–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã:**")
            for obj in latest_result['created_objects']:
                with st.expander(f"{obj['name']} ({obj['type']})"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"**ID:** {obj['id']}")
                        st.write(f"**–¢–∏–ø:** {obj['type']}")
                        st.write(f"**–†–µ–∞–ª—å–Ω–æ—Å—Ç—å:** {obj['reality']}")
                    with col2:
                        st.write(f"**–í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è:** {obj['timestamp']}")
                        st.write(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {obj['confidence']}")
                    
                    if obj['fields']:
                        st.write("**–ü–æ–ª—è:**")
                        fields_data = []
                        for field_name, field_data in obj['fields'].items():
                            fields_data.append({
                                '–ù–∞–∑–≤–∞–Ω–∏–µ': field_name,
                                '–ó–Ω–∞—á–µ–Ω–∏–µ': field_data['value'],
                                '–¢–∏–ø': field_data['type'],
                                '–§–æ—Ä–º—É–ª–∞': field_data.get('is_formula', False)
                            })
                        if fields_data:
                            st.dataframe(pd.DataFrame(fields_data), use_container_width=True)
        
        # –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
        if latest_result['resolved_questions']:
            st.write("**üí° –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã:**")
            for q in latest_result['resolved_questions']:
                st.success(f"**Q:** {q['question']}")
                st.info(f"**A:** {q['answer']}")

def analyze_objects_tab():
    st.header("üîç –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–∫—Ç–æ–≤")
    
    if not st.session_state.controller.objects:
        st.info("–ù–µ—Ç –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –û–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–≤–æ–π –≤–∫–ª–∞–¥–∫–µ.")
        return
    
    # –§–∏–ª—å—Ç—Ä—ã
    col1, col2, col3 = st.columns(3)
    
    with col1:
        type_filter = st.selectbox(
            "–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É:",
            options=["–í—Å–µ"] + [t.value for t in RequestType]
        )
    
    with col2:
        reality_filter = st.selectbox(
            "–§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏:",
            options=["–í—Å–µ"] + [r.value for r in RealityType]
        )
    
    with col3:
        sort_by = st.selectbox(
            "–°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ:",
            options=["–í—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è", "–ù–∞–∑–≤–∞–Ω–∏—é", "–¢–∏–ø—É"]
        )
    
    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
    filtered_objects = []
    for obj_id, obj in st.session_state.controller.objects.items():
        if type_filter != "–í—Å–µ" and obj.request_type.value != type_filter:
            continue
        if reality_filter != "–í—Å–µ" and obj.reality_type.value != reality_filter:
            continue
        filtered_objects.append(obj)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
    if sort_by == "–í—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è":
        filtered_objects.sort(key=lambda x: x.timestamp, reverse=True)
    elif sort_by == "–ù–∞–∑–≤–∞–Ω–∏—é":
        filtered_objects.sort(key=lambda x: x.name)
    elif sort_by == "–¢–∏–ø—É":
        filtered_objects.sort(key=lambda x: x.request_type.value)
    
    st.write(f"**–ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤:** {len(filtered_objects)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—ä–µ–∫—Ç—ã
    for obj in filtered_objects:
        with st.expander(f"üéØ {obj.name} ({obj.request_type.value})"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**ID:** {obj.id}")
                st.write(f"**–¢–∏–ø:** {obj.request_type.value}")
                st.write(f"**–†–µ–∞–ª—å–Ω–æ—Å—Ç—å:** {obj.reality_type.value}")
                st.write(f"**–í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è:** {obj.timestamp}")
                st.write(f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {obj.confidence}")
            
            with col2:
                if obj.subject:
                    st.write(f"**–°—É–±—ä–µ–∫—Ç:** {obj.subject}")
                if obj.target:
                    st.write(f"**–¶–µ–ª—å:** {obj.target}")
                if obj.result:
                    st.write(f"**–†–µ–∑—É–ª—å—Ç–∞—Ç:** {obj.result}")
                if obj.answer:
                    st.write(f"**–û—Ç–≤–µ—Ç:** {obj.answer}")
            
            # –ü–æ–ª—è –æ–±—ä–µ–∫—Ç–∞
            if obj.fields:
                st.write("**–ü–æ–ª—è:**")
                fields_data = []
                for field_name, field_value in obj.fields.items():
                    fields_data.append({
                        '–ù–∞–∑–≤–∞–Ω–∏–µ': field_name,
                        '–ó–Ω–∞—á–µ–Ω–∏–µ': field_value.value,
                        '–¢–∏–ø': field_value.field_type.value,
                        '–§–æ—Ä–º—É–ª–∞': field_value.is_formula,
                        '–ï–¥–∏–Ω–∏—Ü—ã': field_value.unit or "-",
                        '–ù–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å': field_value.uncertainty or "-"
                    })
                st.dataframe(pd.DataFrame(fields_data), use_container_width=True)
            
            # –°–≤—è–∑–∏
            if obj.parents or obj.children or obj.related:
                st.write("**–°–≤—è–∑–∏:**")
                if obj.parents:
                    st.write(f"- –†–æ–¥–∏—Ç–µ–ª–∏: {', '.join(obj.parents)}")
                if obj.children:
                    st.write(f"- –î–µ—Ç–∏: {', '.join(obj.children)}")
                if obj.related:
                    st.write(f"- –°–≤—è–∑–∞–Ω–Ω—ã–µ: {', '.join(obj.related)}")

def integrated_system_tab():
    st.header("‚ö° –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞")
    
    st.markdown("""
    –≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç:
    - üß† –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —è–∑—ã–∫–æ–≤–æ–π –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä
    - üî¢ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
    - ‚öôÔ∏è –ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –∫–æ–º–∞–Ω–¥
    """)
    
    user_input = st.text_area(
        "–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏:",
        height=100,
        placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: –Ø —Ö–æ—á—É —Å–æ–∑–¥–∞—Ç—å —Å—Ç–∞—Ä—Ç–∞–ø –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –ò–ò"
    )
    
    if st.button("üöÄ –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞", type="primary"):
        if user_input.strip():
            with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É..."):
                try:
                    result = st.session_state.integrated_system.process_input(user_input)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.subheader("üìä –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
                        st.json(result['universal_analysis'])
                        
                        st.subheader("üî¢ –í–µ–∫—Ç–æ—Ä (–ø–µ—Ä–≤—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
                        vector_data = result['vector'][:5] if len(result['vector']) > 5 else result['vector']
                        st.bar_chart(pd.DataFrame({'–ó–Ω–∞—á–µ–Ω–∏–µ': vector_data}))
                    
                    with col2:
                        st.subheader("‚öôÔ∏è –ö–æ–º–∞–Ω–¥—ã Infera")
                        st.code(result['infera_commands'])
                        
                        st.subheader("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
                        execution = result['execution_result']
                        st.write(f"**–°—Ç–∞—Ç—É—Å:** {'‚úÖ –£—Å–ø–µ—à–Ω–æ' if execution['success'] else '‚ùå –û—à–∏–±–∫–∞'}")
                        st.write(f"**–°–æ–∑–¥–∞–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤:** {execution['created']}")
                        st.write(f"**–ò–∑–º–µ–Ω–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤:** {execution['modified']}")
                    
                    # –û—Ç–≤–µ—Ç—ã
                    if result['answers']:
                        st.subheader("üí° –û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã")
                        for answer in result['answers']:
                            st.success(f"**Q:** {answer['question']}")
                            st.info(f"**A:** {answer['answer']}")
                    
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")
                    st.exception(e)

def visualization_tab():
    st.header("üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è")
    
    if not st.session_state.controller.objects:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏. –û–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–≤–æ–π –≤–∫–ª–∞–¥–∫–µ.")
        return
    
    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
    st.subheader("‚è∞ –í—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤")
    
    timeline_data = []
    for obj_id, obj in st.session_state.controller.objects.items():
        timeline_data.append({
            '–í—Ä–µ–º—è': obj.timestamp,
            '–û–±—ä–µ–∫—Ç': obj.name,
            '–¢–∏–ø': obj.request_type.value,
            '–†–µ–∞–ª—å–Ω–æ—Å—Ç—å': obj.reality_type.value
        })
    
    if timeline_data:
        timeline_df = pd.DataFrame(timeline_data)
        timeline_df = timeline_df.sort_values('–í—Ä–µ–º—è')
        
        fig_timeline = px.scatter(
            timeline_df,
            x='–í—Ä–µ–º—è',
            y='–û–±—ä–µ–∫—Ç',
            color='–¢–∏–ø',
            symbol='–†–µ–∞–ª—å–Ω–æ—Å—Ç—å',
            title='–í—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤'
        )
        st.plotly_chart(fig_timeline, use_container_width=True)
    
    # –ì—Ä–∞—Ñ —Å–≤—è–∑–µ–π
    st.subheader("üï∏Ô∏è –ì—Ä–∞—Ñ —Å–≤—è–∑–µ–π –æ–±—ä–µ–∫—Ç–æ–≤")
    
    nodes = []
    edges = []
    
    for obj_id, obj in st.session_state.controller.objects.items():
        nodes.append({
            'id': obj_id,
            'label': obj.name,
            'type': obj.request_type.value,
            'reality': obj.reality_type.value
        })
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∏
        for parent_id in obj.parents:
            edges.append({'from': parent_id, 'to': obj_id, 'relation': 'parent'})
        for child_id in obj.children:
            edges.append({'from': obj_id, 'to': child_id, 'relation': 'child'})
        for related_id in obj.related:
            edges.append({'from': obj_id, 'to': related_id, 'relation': 'related'})
    
    if nodes:
        # –°–æ–∑–¥–∞—ë–º –≥—Ä–∞—Ñ —Å –ø–æ–º–æ—â—å—é Plotly
        fig_graph = go.Figure()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã
        node_trace = go.Scatter(
            x=[i for i in range(len(nodes))],
            y=[0 for _ in nodes],
            mode='markers+text',
            text=[node['label'] for node in nodes],
            textposition="middle center",
            marker=dict(size=30, color='lightblue'),
            name='–û–±—ä–µ–∫—Ç—ã'
        )
        
        fig_graph.add_trace(node_trace)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤—è–∑–∏
        for edge in edges:
            # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏ —É–∑–ª–æ–≤
            from_idx = next((i for i, node in enumerate(nodes) if node['id'] == edge['from']), None)
            to_idx = next((i for i, node in enumerate(nodes) if node['id'] == edge['to']), None)
            
            if from_idx is not None and to_idx is not None:
                fig_graph.add_trace(go.Scatter(
                    x=[from_idx, to_idx],
                    y=[0, 0],
                    mode='lines',
                    line=dict(color='gray', width=2),
                    showlegend=False
                ))
        
        fig_graph.update_layout(
            title='–ì—Ä–∞—Ñ —Å–≤—è–∑–µ–π –æ–±—ä–µ–∫—Ç–æ–≤',
            showlegend=True,
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
        )
        
        st.plotly_chart(fig_graph, use_container_width=True)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—è–º
    st.subheader("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–µ–π")
    
    field_stats = {}
    for obj_id, obj in st.session_state.controller.objects.items():
        for field_name, field_value in obj.fields.items():
            if field_name not in field_stats:
                field_stats[field_name] = {
                    'count': 0,
                    'types': set(),
                    'has_formula': 0
                }
            field_stats[field_name]['count'] += 1
            field_stats[field_name]['types'].add(field_value.field_type.value)
            if field_value.is_formula:
                field_stats[field_name]['has_formula'] += 1
    
    if field_stats:
        stats_data = []
        for field_name, stats in field_stats.items():
            stats_data.append({
                '–ü–æ–ª–µ': field_name,
                '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': stats['count'],
                '–¢–∏–ø—ã': ', '.join(stats['types']),
                '–° —Ñ–æ—Ä–º—É–ª–∞–º–∏': stats['has_formula']
            })
        
        st.dataframe(pd.DataFrame(stats_data), use_container_width=True)

def process_user_input(text):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –≤–≤–æ–¥"""
    with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é..."):
        try:
            result = st.session_state.controller.process_input(text)
            st.session_state.processing_history.append(result)
            st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            st.rerun()
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {str(e)}")
            st.exception(e)

def create_temporal_chains():
    """–°–æ–∑–¥–∞—ë—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤"""
    objects = [obj for obj in st.session_state.controller.objects.values() 
               if obj.request_type == RequestType.OBJECT]
    
    if not objects:
        st.warning("–ù–µ—Ç –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫")
        return
    
    # –°–æ–∑–¥–∞—ë–º –±—É–¥—É—â–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤
    created_chains = 0
    for obj in objects:
        if obj.name == "–Ø":
            # –°–æ–∑–¥–∞—ë–º –±—É–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            future_obj = st.session_state.controller.create_temporal_chain(
                obj.id,
                {
                    "future_state": FieldValue(
                        value="improved",
                        field_type=FieldType.STATE
                    )
                }
            )
            created_chains += 1
        elif "–±–∏–∑–Ω–µ—Å" in obj.name.lower():
            # –°–æ–∑–¥–∞—ë–º –±—É–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–∏–∑–Ω–µ—Å–∞
            future_obj = st.session_state.controller.create_temporal_chain(
                obj.id,
                {
                    "growth": FieldValue(
                        value=1.5,
                        field_type=FieldType.CUSTOM
                    )
                }
            )
            created_chains += 1
    
    if created_chains > 0:
        st.success(f"–°–æ–∑–¥–∞–Ω–æ {created_chains} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫")
        st.rerun()
    else:
        st.info("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ü–µ–ø–æ—á–µ–∫")

def resolve_questions():
    """–†–∞–∑—Ä–µ—à–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –≤ —Å–∏—Å—Ç–µ–º–µ"""
    questions = [obj for obj in st.session_state.controller.objects.values() 
                 if obj.request_type == RequestType.QUESTION and not obj.answer]
    
    if not questions:
        st.info("–ù–µ—Ç –Ω–µ—Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
        return
    
    resolved_count = 0
    for question in questions:
        # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤
        if "–∫–∞–∫" in question.name.lower():
            question.answer = "–ù—É–∂–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –ø–ª–∞–Ω –∏ —Å–ª–µ–¥–æ–≤–∞—Ç—å –µ–º—É –ø–æ—ç—Ç–∞–ø–Ω–æ"
            resolved_count += 1
        elif "—Å–∫–æ–ª—å–∫–æ" in question.name.lower():
            question.answer = "–ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π"
            resolved_count += 1
        elif "—á—Ç–æ" in question.name.lower():
            question.answer = "–≠—Ç–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Ü–µ–ª–µ–π"
            resolved_count += 1
    
    if resolved_count > 0:
        st.success(f"–†–∞–∑—Ä–µ—à–µ–Ω–æ {resolved_count} –≤–æ–ø—Ä–æ—Å–æ–≤")
        st.rerun()
    else:
        st.info("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑—Ä–µ—à–∏—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")

def training_tab():
    """–í–∫–ª–∞–¥–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π"""
    st.header("üß† –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π")
    st.markdown("**–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ AGI —Å–∏—Å—Ç–µ–º—ã**")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
    if 'training_history' not in st.session_state:
        st.session_state.training_history = []
    if 'training_in_progress' not in st.session_state:
        st.session_state.training_in_progress = False
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è")
        
        # –í—ã–±–æ—Ä –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
        network_type = st.selectbox(
            "üß† –ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:",
            ["–î–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω–∞—è (–≤—Ç–æ—Ä–∞—è) - CommandNetwork"],
            help="–ü–æ–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –≤—Ç–æ—Ä–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å"
        )
        
        # –í—ã–±–æ—Ä –º–∞—Ç–µ—Ä–∏–∞–ª–∞
        material_type = st.selectbox(
            "üìö –¢–∏–ø –æ–±—É—á–∞—é—â–µ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞:",
            ["–ê—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞ (–¥–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)"],
            help="–ü–æ–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞ –≤ –¥–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        st.markdown("**üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞–Ω–Ω—ã—Ö:**")
        num_samples = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤:", 100, 2000, 500, 100)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        st.markdown("**üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:**")
        num_epochs = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö:", 1, 20, 4)
        batch_size = st.slider("–†–∞–∑–º–µ—Ä batch:", 2, 16, 4)
        learning_rate = st.select_slider(
            "Learning rate:", 
            options=[1e-5, 5e-5, 1e-4, 5e-4, 1e-3],
            value=5e-5,
            format_func=lambda x: f"{x:.0e}"
        )
        
        # –ö–Ω–æ–ø–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        if st.button("üìã –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö", disabled=st.session_state.training_in_progress):
            with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ..."):
                # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
                import sys
                sys.path.append('.')
                from arithmetic_declarative_generator import ArithmeticDeclarativeGenerator
                
                generator = ArithmeticDeclarativeGenerator()
                dataset = generator.generate_dataset(num_samples)
                
                st.session_state.generated_dataset = dataset
                st.success(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤!")
        
        # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        if st.button("üöÄ –ù–ê–ß–ê–¢–¨ –û–ë–£–ß–ï–ù–ò–ï", 
                     type="primary", 
                     disabled=st.session_state.training_in_progress or 'generated_dataset' not in st.session_state):
            
            st.session_state.training_in_progress = True
            training_placeholder = st.empty()
            progress_bar = st.progress(0)
            
            # –†–ï–ê–õ–¨–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ù–ï–ô–†–û–°–ï–¢–ò
            training_placeholder.info("üì• –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            import tempfile
            import os
            
            # –ß–ï–°–¢–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: 2 –ù–ê–°–¢–û–Ø–©–ò–ï –ù–ï–ô–†–û–°–ï–¢–ò (768D –∫–∞–∂–¥–∞—è)
            training_data = []
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—É—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å (Encoder)
            from vectorizer import get_vector  # all-mpnet-base-v2, 768D, 109M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            
            for sample in st.session_state.generated_dataset:
                # 1-—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å: text ‚Üí vector[768] (SentenceTransformer - –ù–ê–°–¢–û–Ø–©–ê–Ø)
                input_text = sample['text']  # "—Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 17 –ø–ª—é—Å 6 –ø–ª—é—Å 19"
                input_vector = get_vector(input_text)  # [768] - —Ä–µ–∞–ª—å–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –≤–µ–∫—Ç–æ—Ä
                
                output_text = "\n".join(sample['declarations'])
                training_data.append({
                    'text': input_text,
                    'vector': input_vector,  # 768-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –æ—Ç –Ω–∞—Å—Ç–æ—è—â–µ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
                    'declarations_text': output_text
                })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False, encoding='utf-8')
            for item in training_data:
                temp_file.write(json.dumps(item, ensure_ascii=False) + '\n')
            temp_file.close()
            
            try:
                training_placeholder.info("üß† –û–±—É—á–∞–µ–º 2-—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å: Vector[768] ‚Üí Declarations...")
                
                # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                import torch
                import torch.nn as nn
                from torch.utils.data import Dataset
                from transformers import T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments
                
                # –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è vector-to-text –æ–±—É—á–µ–Ω–∏—è
                class VectorToTextDataset(Dataset):
                    def __init__(self, data_path, tokenizer, max_length=256):
                        self.tokenizer = tokenizer
                        self.max_length = max_length
                        self.vectors = []
                        self.targets = []
                        
                        with open(data_path, 'r', encoding='utf-8') as f:
                            for line in f:
                                sample = json.loads(line)
                                self.vectors.append(sample['vector'])  # 768D –≤–µ–∫—Ç–æ—Ä –æ—Ç 1-–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
                                self.targets.append(sample['declarations_text'])
                    
                    def __len__(self):
                        return len(self.vectors)
                    
                    def __getitem__(self, idx):
                        vector = torch.tensor(self.vectors[idx], dtype=torch.float32)  # [768]
                        target_text = self.targets[idx]
                        
                        # –≠–Ω–∫–æ–¥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤–æ–π —Ç–µ–∫—Å—Ç
                        target_encoding = self.tokenizer(
                            target_text, max_length=self.max_length, 
                            padding='max_length', truncation=True, return_tensors='pt'
                        )
                        
                        labels = target_encoding['input_ids']
                        labels[labels == self.tokenizer.pad_token_id] = -100
                        
                        return {
                            'input_vector': vector,  # 768D –≤–µ–∫—Ç–æ—Ä
                            'labels': labels.flatten()
                        }
                
                # 2-—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å: Vector[768] ‚Üí Text (T5-based)
                class VectorToTextTransformer(nn.Module):
                    def __init__(self, t5_model, vector_size=768):
                        super().__init__()
                        self.t5_model = t5_model
                        self.vector_size = vector_size
                        
                        # –õ–∏–Ω–µ–π–Ω—ã–π –ø—Ä–æ–µ–∫—Ç–æ—Ä: vector[768] ‚Üí T5_embedding[512]
                        self.vector_projector = nn.Linear(vector_size, t5_model.config.d_model)
                        
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤
                        self.vector_processor = nn.Sequential(
                            nn.Linear(vector_size, 1024),
                            nn.ReLU(),
                            nn.Dropout(0.1),
                            nn.Linear(1024, t5_model.config.d_model)
                        )
                        
                        # –ó–∞–º–æ—Ä–æ–∑–∏–º T5 encoder, –æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ decoder + –Ω–∞—à–∏ —Å–ª–æ–∏
                        for param in self.t5_model.encoder.parameters():
                            param.requires_grad = False
                    
                    def forward(self, input_vector=None, labels=None, **kwargs):
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º 768D –≤–µ–∫—Ç–æ—Ä —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —Å–ª–æ–∏
                        processed_vector = self.vector_processor(input_vector)  # [768] ‚Üí [512]
                        
                        # –°–æ–∑–¥–∞—ë–º encoder_outputs –¥–ª—è T5 decoder
                        batch_size = processed_vector.shape[0]
                        encoder_outputs = type('obj', (object,), {
                            'last_hidden_state': processed_vector.unsqueeze(1)  # [batch, 1, 512]
                        })()
                        
                        # T5 decoder –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞
                        return self.t5_model(
                            encoder_outputs=encoder_outputs,
                            labels=labels,
                            **kwargs
                        )
                    
                    def generate(self, input_vector, **kwargs):
                        processed_vector = self.vector_processor(input_vector)
                        encoder_outputs = type('obj', (object,), {
                            'last_hidden_state': processed_vector.unsqueeze(1)
                        })()
                        
                        return self.t5_model.generate(
                            encoder_outputs=encoder_outputs,
                            **kwargs
                        )
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é T5 –º–æ–¥–µ–ª—å
                training_placeholder.info("üì• –°–æ–∑–¥–∞–µ–º 2-—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å (Vector‚ÜíText Transformer)...")
                tokenizer = T5Tokenizer.from_pretrained('t5-small', legacy=False)
                base_t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')
                
                # –°–æ–∑–¥–∞—ë–º –≤—Ç–æ—Ä—É—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å
                model = VectorToTextTransformer(base_t5_model, vector_size=768)
                
                # –ü–æ–¥—Å—á–∏—Ç–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                
                training_placeholder.info(f"üìä 2-—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å: {trainable_params:,} –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
                
                # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç
                dataset = VectorToTextDataset(temp_file.name, tokenizer)
                train_size = int(0.9 * len(dataset))
                eval_size = len(dataset) - train_size
                train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, eval_size])
                
                # –ö–∞—Å—Ç–æ–º–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è vector-to-text
                class VectorToTextTrainer(Seq2SeqTrainer):
                    def compute_loss(self, model, inputs, return_outputs=False):
                        input_vector = inputs.get('input_vector')
                        labels = inputs.get('labels')
                        
                        outputs = model(input_vector=input_vector, labels=labels)
                        loss = outputs.loss
                        
                        return (loss, outputs) if return_outputs else loss
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
                training_args = Seq2SeqTrainingArguments(
                    output_dir='./temp_model',
                    num_train_epochs=num_epochs,
                    per_device_train_batch_size=batch_size,
                    per_device_eval_batch_size=batch_size,
                    warmup_steps=10,
                    weight_decay=0.01,
                    logging_steps=max(1, len(train_dataset) // (batch_size * 4)),
                    eval_strategy="epoch",
                    save_strategy="no",
                    predict_with_generate=True,
                )
                
                # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è vector-to-text
                def compute_metrics(p):
                    return {"training_loss": p.predictions.mean() if hasattr(p, 'predictions') else 0}
                
                # Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è UI
                class StreamlitCallback:
                    def __init__(self, placeholder, progress_bar, history_list):
                        self.placeholder = placeholder
                        self.progress_bar = progress_bar
                        self.history_list = history_list
                        self.current_epoch = 0
                    
                    def on_epoch_end(self, args, state, control, **kwargs):
                        self.current_epoch += 1
                        progress = self.current_epoch / num_epochs
                        self.progress_bar.progress(progress)
                        
                        if state.log_history:
                            latest_log = state.log_history[-1]
                            loss = latest_log.get('train_loss', 0)
                            
                            epoch_metrics = {
                                'epoch': self.current_epoch,
                                'loss': loss,
                                'accuracy': max(0, 1 - loss),  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                                'exact_match': max(0, 1 - loss)
                            }
                            self.history_list.append(epoch_metrics)
                            
                            self.placeholder.info(f"–≠–ø–æ—Ö–∞ {self.current_epoch}/{num_epochs} - Loss: {loss:.3f}")
                
                # –°–æ–∑–¥–∞—ë–º —Ç—Ä–µ–Ω–µ—Ä
                trainer = VectorToTextTrainer(
                    model=model,
                    args=training_args,
                    train_dataset=train_dataset,
                    eval_dataset=eval_dataset,
                    processing_class=tokenizer,
                    compute_metrics=compute_metrics,
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º callback
                callback = StreamlitCallback(training_placeholder, progress_bar, st.session_state.training_history)
                trainer.add_callback(callback)
                
                # –ó–ê–ü–£–°–ö–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï 2-–ô –ù–ï–ô–†–û–°–ï–¢–ò
                training_placeholder.info("üöÄ –û–±—É—á–∞–µ–º 2-—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å: Vector[768] ‚Üí Declarations...")
                trainer.train()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                model_save_path = './models/vector_to_text_network'
                os.makedirs(model_save_path, exist_ok=True)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ç–æ—Ä—É—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å
                torch.save({
                    'vector_processor': model.vector_processor.state_dict(),
                    'decoder_state': model.t5_model.decoder.state_dict(),
                    'config': model.t5_model.config,
                    'vector_size': 768,
                    'total_params': total_params,
                    'trainable_params': trainable_params
                }, os.path.join(model_save_path, 'vector_to_text_model.pth'))
                
                tokenizer.save_pretrained(model_save_path)
                
                st.session_state.training_in_progress = False
                training_placeholder.success(f"‚úÖ 2-—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –æ–±—É—á–µ–Ω–∞! {trainable_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
                
            except Exception as e:
                st.session_state.training_in_progress = False
                training_placeholder.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {str(e)}")
            
            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if os.path.exists(temp_file.name):
                    os.unlink(temp_file.name)

    with col2:
        st.subheader("üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è")
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
        if st.session_state.training_history:
            df_history = pd.DataFrame(st.session_state.training_history)
            
            # –ì—Ä–∞—Ñ–∏–∫ —Å –¥–≤—É–º—è –æ—Å—è–º–∏
            fig = make_subplots(
                rows=2, cols=1,
                subplot_titles=["–¢–æ—á–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è", "–ü–æ—Ç–µ—Ä–∏ (Loss)"],
                vertical_spacing=0.1
            )
            
            # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
            fig.add_trace(
                go.Scatter(
                    x=df_history['epoch'],
                    y=df_history['accuracy'],
                    name="–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å",
                    line=dict(color='green')
                ),
                row=1, col=1
            )
            
            fig.add_trace(
                go.Scatter(
                    x=df_history['epoch'],
                    y=df_history['exact_match'],
                    name="–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ",
                    line=dict(color='blue')
                ),
                row=1, col=1
            )
            
            # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
            fig.add_trace(
                go.Scatter(
                    x=df_history['epoch'],
                    y=df_history['loss'],
                    name="Loss",
                    line=dict(color='red')
                ),
                row=2, col=1
            )
            
            fig.update_layout(height=500, showlegend=True)
            fig.update_xaxes(title_text="–≠–ø–æ—Ö–∞")
            fig.update_yaxes(title_text="–¢–æ—á–Ω–æ—Å—Ç—å", row=1, col=1)
            fig.update_yaxes(title_text="Loss", row=2, col=1)
            
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("üìà –ì—Ä–∞—Ñ–∏–∫ –ø–æ—è–≤–∏—Ç—Å—è –ø–æ—Å–ª–µ –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è")
        
        # –¢–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        if st.session_state.training_history:
            latest_metrics = st.session_state.training_history[-1]
            
            col_m1, col_m2, col_m3 = st.columns(3)
            with col_m1:
                st.metric("–¢–æ—á–Ω–æ—Å—Ç—å", f"{latest_metrics['accuracy']:.3f}")
            with col_m2:
                st.metric("Exact Match", f"{latest_metrics['exact_match']:.3f}")
            with col_m3:
                st.metric("Loss", f"{latest_metrics['loss']:.3f}")

    # –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    st.subheader("üìã –ü—Ä–∏–º–µ—Ä—ã –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    if 'generated_dataset' in st.session_state:
        st.markdown("**üß† –ß–ï–°–¢–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: 2 –ù–ê–°–¢–û–Ø–©–ò–ï –ù–ï–ô–†–û–°–ï–¢–ò (768D):**")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤
        for i, sample in enumerate(st.session_state.generated_dataset[:5]):
            with st.expander(f"–ü—Ä–∏–º–µ—Ä {i+1}: {sample['text']}"):
                st.markdown("**üìù –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç:**")
                st.code(sample['text'])
                
                st.markdown("**1Ô∏è‚É£ 1-—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å (Encoder): text ‚Üí vector[768]**")
                st.code("SentenceTransformer 'all-mpnet-base-v2' (109M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) ‚Üí [0.123, -0.456, 0.789, ...768 —á–∏—Å–µ–ª]")
                
                st.markdown("**2Ô∏è‚É£ 2-—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å (Decoder): vector[768] ‚Üí declarations**")
                st.code("VectorToTextTransformer (–æ–±—É—á–∞–µ–º–∞—è) ‚Üí –¥–µ–∫–ª–∞—Ä–∞—Ü–∏–∏")
                
                st.markdown("**üìã –†–µ–∑—É–ª—å—Ç–∞—Ç (–¥–µ–∫–ª–∞—Ä–∞—Ü–∏–∏):**")
                declarations_text = "\n".join(sample['declarations'])
                st.code(declarations_text)
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                st.markdown("**üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞:**")
                objects_count = len([d for d in sample['declarations'] if 'OBJECT' in d])
                intents_count = len([d for d in sample['declarations'] if 'INTENT' in d])
                
                col_s1, col_s2, col_s3 = st.columns(3)
                with col_s1:
                    st.metric("–û–±—ä–µ–∫—Ç—ã", objects_count)
                with col_s2:
                    st.metric("–ù–∞–º–µ—Ä–µ–Ω–∏—è", intents_count)
                with col_s3:
                    st.metric("–ù–µ–π—Ä–æ—Å–µ—Ç–∏", "2 –ù–ê–°–¢–û–Ø–©–ò–ï")
    else:
        st.info("üëÜ –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø—Ä–∏–º–µ—Ä–æ–≤")

    # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏
    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è"):
        st.session_state.training_history = []
        if 'generated_dataset' in st.session_state:
            del st.session_state.generated_dataset
        st.success("–ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞!")
        st.rerun()

if __name__ == "__main__":
    main()